{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74dcddee",
   "metadata": {},
   "source": [
    "# Acelerando aplicações com CUDA C/C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763f959",
   "metadata": {},
   "source": [
    "* CUDA é um acrônimo para Compute Unified Device Architecture.\n",
    "* É uma API criada pela NVIDIA para Computação Paralela em suas GPUS, portanto precisamos de uma GPU da NVIDIA para programar em CUDA. Entretanto, existem ferramentas que tentam integrar GPU's da AMD com o código CUDA, como por exemplo: [ZLUDA](https://github.com/vosen/ZLUDA?authuser=0)\n",
    "* Um sistema acelerado, também conhecido como sistema heterogêneo, são sistemas compostos por uma CPU e uma GPU;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70561ac",
   "metadata": {},
   "source": [
    "* Para obter informações da sua GPU NVIDIA que rodará seu código CUDA, utilize a CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abebca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e5a40a",
   "metadata": {},
   "source": [
    "# CPU vs GPU\n",
    "* Códigos rodados apenas na CPU geralmente são serializados, isto é, a chamada de uma função fará uma série de instruções sequencialmente. Quando trabalhamos com CUDA, podemos ter processamento em paralelo nos dados por parte da GPU, enquanto a CPU trabalha com outros dados, o que nos faz ganhar tempo. Para armazenar dados na GPU e ganharmos acesso a este processamento paralelo, chamamos cudaMallocManaged(). É importante ressaltar que trabalhar na GPU é um processo assíncrono, já que a CPU trabalha ao mesmo tempo. Portanto, para sincronizá-los, chamamos cudaDeviceSynchronize() para que a CPU seja sincronizada com a GPU, isto é, aguardar a GPU terminar o trabalho dela. Depois disso podemos por exemplo acessar os dados processados pela GPU na CPU novamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae0fd91",
   "metadata": {},
   "source": [
    "# Escrevendo códigos para GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed19792",
   "metadata": {},
   "source": [
    "* .cu é a extensão para arquivos CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65480fc5",
   "metadata": {},
   "source": [
    "```cpp\n",
    "void CPUFunction()\n",
    "{\n",
    "  printf(\"This function is defined to run on the CPU.\\n\");\n",
    "}\n",
    "\n",
    "__global__ void GPUFunction()\n",
    "{\n",
    "  printf(\"This function is defined to run on the GPU.\\n\");\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  CPUFunction();\n",
    "\n",
    "  GPUFunction<<<1, 1>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6036b",
   "metadata": {},
   "source": [
    "* O código acima possui duas funções: a primeira que será rodada na CPU e a segunda que será executada na GPU.\n",
    "* A palavra-chave ```__global__``` indica que a função será rodada na GPU e pode ser invocada globalmente, o que neste contexto significa CPU e GPU.\n",
    "* Geralmente, o código rodado na CPU é chamado de **HOST** code e o código rodado na GPU é chamado de **DEVICE** code.\n",
    "* Quando uma função é chamada para ser executada na GPU, isto é, possui o pre-fixo __global__, chamamos esta função de **kernel** que é **inicializado/launched**.\n",
    "* ``` GPUFunction<<<1, 1>>>(); ```\n",
    "* <<< ... >>> através desta sintaxe colocamos as configurações de execução deste kernel, ou função da GPU, o primeiro parâmetro é o número de blocos e o segundo parâmetro é a quantidade de threads em cada bloco.\n",
    "* ``` cudaDeviceSynchronize(); ```\n",
    "* Diferente do código padrão de C/C++, inicializar kernels é uma tarefa assíncrona: o código da CPU continuará executando sem esperar o kernel finalizar.\n",
    "* A chamada para ``` cudaDeviceSynchronize() ``` faz com que a CPU espere até o código executado na GPU termine para continuar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f81cbbd",
   "metadata": {},
   "source": [
    "# NVCC: NVIDIA CUDA Compiler\n",
    "* Para compilar e roda seu programa em CUDA, utilizamos o ```nvcc``` que é bem parecido com o ```gcc```:\n",
    "```nvcc -o out some-CUDA.cu -run```\n",
    "* em que:\n",
    "* some-CUDA.cu é o nosso arquivo CUDA\n",
    "* out será o nome do nosso programa compilado\n",
    "* run para executá-lo imediatamente\n",
    "* OBS: para códigos não CUDA (host code), o nvcc chama o gcc para compilar, trabalhando apenas nos códigos CUDA (device code) (__global__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f28204",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o hello-gpu module1/hello-gpu.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f1233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
